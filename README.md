## Screenshots
![chat_groq_1](https://github.com/Parthiban-R-3997/Smart-ATS-System-Using-Google-Gemini/assets/26496805/6125958f-93ac-4399-aa86-55a9f2613801)
![chat_groq_2](https://github.com/Parthiban-R-3997/Smart-ATS-System-Using-Google-Gemini/assets/26496805/d46cb54c-c7ce-43c2-9ad1-6530ce148366)
![chat_groq_5](https://github.com/Parthiban-R-3997/Smart-ATS-System-Using-Google-Gemini/assets/26496805/c0fc5328-cad2-4e13-9b61-5323fb653305)
![chat_groq_6](https://github.com/Parthiban-R-3997/Smart-ATS-System-Using-Google-Gemini/assets/26496805/574bd55a-0726-48b3-aff7-e6d9ddfd874c)

# Chat Groq Document Q&A

This Streamlit web application allows you to upload PDF documents, embed them using Google's Generative AI Embeddings, and perform question-answering based on the uploaded documents. The app utilizes the powerful Groq language model and its efficient inference engine to provide accurate and relevant answers to your queries. The application follows the RAG (Retrieval-Augmented Generation) approach, which combines the strengths of retrieval systems and generative language models.

## Features

- **PDF Document Upload**: Upload multiple PDF files to create a knowledge base.
- **Document Embedding**: Leverage Google's Generative AI Embeddings to embed the uploaded PDF documents.
- **Question Answering**: Ask questions related to the uploaded documents, and receive precise answers powered by the Groq's LPU inference engine.
- **Custom Prompt Templates**: Customize the prompt template for question-answering to suit your specific needs.
- **Model Selection**: Choose from a variety of available Groq open source models, including `llama3-8b-8192`, `llama3-70b-8192`, `mixtral-8x7b-32768`, and `gemma-7b-it`.
- **Document Similarity Search**: Explore relevant document chunks that match the provided question.
- **Response Time Tracking**: Monitor the response time for each query.

## RAG (Retrieval-Augmented Generation) Approach

The application follows the RAG approach, which combines the strengths of retrieval systems and generative language models. Here's how it works:

- **Retrieval**: The application embeds the uploaded PDF documents using Google's Generative AI Embeddings and stores them in a vector store (FAISS). When a user asks a question, the relevant document chunks are retrieved from the vector store based on their similarity to the question.

- **Augmentation**: The retrieved document chunks are combined and provided as context to the Groq language model. This augments the language model's knowledge with relevant information from the uploaded documents.

- **Generation**: The Groq language model uses the provided context and the question to generate a precise and relevant answer. The language model's generative capabilities allow it to synthesize information from the retrieved document chunks and produce a coherent response.

The RAG approach offers several advantages:

- **Scalability**: By leveraging a retrieval system, the application can handle large knowledge bases efficiently, allowing users to query information from extensive document collections.
- **Contextual Understanding**: The language model can better understand the context of the question and provide more accurate and nuanced answers by using the relevant document chunks as context.
- **Knowledge Grounding**: The generated answers are grounded in the factual information present in the uploaded documents, ensuring the reliability and trustworthiness of the responses.

## Advantages of Groq LPU Inference Engine

- **High Performance**: Groq's LP (Learning Processor) Inference Engine is designed to deliver exceptional performance for large language models, enabling fast and efficient question answering.
- **Energy Efficiency**: The LP Inference Engine is optimized for energy efficiency, making it suitable for deployment on various devices and environments.
- **Scalability**: Groq's architecture allows for seamless scaling of language models, ensuring that the application can handle increasing demands and larger knowledge bases.
- **Low Latency**: The LP Inference Engine provides low-latency inference, ensuring quick response times for user queries.

## Langsmith Integration

This application integrates with Langsmith, a platform for monitoring and managing large language model deployments. Langsmith provides the following benefits:

- **Log Monitoring**: Monitor and analyze logs generated by the application, making it easier to debug and troubleshoot issues.
- **Cost Optimization**: Langsmith helps optimize the cost of running language models by providing insights into usage patterns and suggesting ways to reduce costs.
- **Performance Monitoring**: Track the performance of the application and identify potential bottlenecks or areas for improvement.


## Getting Started

1. Clone the repository:

  ```bash
  git clone https://github.com/your-username/chat-groq-document-qa.git

2. Install the required dependencies:

  ```bash
  pip install -r requirements.txt

3. Setup Your API Keys

  ```bash
- Obtain a Groq API key from the [Groq Console](https://console.groq.com/keys).
- Obtain a Google API key from the [Google AI Studio](https://aistudio.google.com/app/apikey).


4. Run the Streamlit App

```bash
streamlit run app.py


5. Run the Streamlit app:
```bash
streamlit run app.py

Open the app in your web browser and follow the instructions to upload PDF files, enter your API keys, select a Groq model, and start asking questions!

## Usage

->In the sidebar, enter your Groq API key and Google API key.
->Upload one or more PDF files containing the information you want to query.
->(Optional) Customize the prompt template for question-answering.
->Select the desired Groq model from the available options.
->Click the "Start Document Embedding" button to embed the uploaded documents.
->Once the embedding process is complete, enter your question in the text area.
->The app will provide an answer based on the uploaded documents, along with relevant document chunks and response time.

## Contributing
Contributions are welcome! If you find any issues or have suggestions for improvements, please open an issue or submit a pull request.
License
This project is licensed under the MIT License.


## Acknowledgments

Streamlit for creating an excellent framework for building data apps.
LangChain for providing a powerful library for building applications with language models.
Groq for their state-of-the-art language models and efficient inference engine.
Google Generative AI for their powerful embedding models.
